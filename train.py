# src/train.py
import os
import torch
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
    DataCollatorWithPadding
)
from datasets import Dataset
from sklearn.metrics import accuracy_score, f1_score
import pandas as pd

# ----------------------------
# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
# ----------------------------
MODEL_NAME = "cointegrated/rubert-tiny2"
MAX_LENGTH = 256
BATCH_SIZE = 8
EPOCHS = 3

SCRIPT_DIR = os.path.dirname(__file__)
DATA_DIR = os.path.join(SCRIPT_DIR, "..", "data", "processed")
MODEL_OUTPUT_DIR = os.path.join(SCRIPT_DIR, "..", "models", "rubert_news_classifier")

# ----------------------------
# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
# ----------------------------
print("üìÇ –ó–∞–≥—Ä—É–∂–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ...")
train_df = pd.read_csv(os.path.join(DATA_DIR, "train.csv"))
val_df = pd.read_csv(os.path.join(DATA_DIR, "val.csv"))

unique_topics = sorted(train_df['topic'].unique())
label2id = {topic: idx for idx, topic in enumerate(unique_topics)}
id2label = {idx: topic for topic, idx in label2id.items()}

print(f"üìö –ö–ª–∞—Å—Å—ã: {unique_topics}")

train_df['label'] = train_df['topic'].map(label2id)
val_df['label'] = val_df['topic'].map(label2id)

# ----------------------------
# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
# ----------------------------
print("‚öôÔ∏è –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ Hugging Face Dataset...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

def tokenize_function(examples):
    return tokenizer(examples["text"], truncation=True, padding=False, max_length=MAX_LENGTH)

train_dataset = Dataset.from_pandas(train_df[["text", "label"]])
val_dataset = Dataset.from_pandas(val_df[["text", "label"]])

train_dataset = train_dataset.map(tokenize_function, batched=True)
val_dataset = val_dataset.map(tokenize_function, batched=True)

train_dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "label"])
val_dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "label"])

# ----------------------------
# –ú–æ–¥–µ–ª—å
# ----------------------------
print(f"ü¶æ –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å {MODEL_NAME}...")
model = AutoModelForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=len(label2id),
    id2label=id2label,
    label2id=label2id
)

# ----------------------------
# –ú–µ—Ç—Ä–∏–∫–∏
# ----------------------------
def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    preds = predictions.argmax(axis=1)
    return {
        "accuracy": accuracy_score(labels, preds),
        "f1_macro": f1_score(labels, preds, average="macro")
    }

# ----------------------------
# –û–±—É—á–µ–Ω–∏–µ –±–µ–∑ evaluation_strategy
# ----------------------------
print("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ...")

training_args = TrainingArguments(
    output_dir=MODEL_OUTPUT_DIR,
    num_train_epochs=1,  # –û–±—É—á–∞–µ–º –ø–æ 1 —ç–ø–æ—Ö–µ –∑–∞ —Ä–∞–∑
    per_device_train_batch_size=BATCH_SIZE,
    per_device_eval_batch_size=BATCH_SIZE,
    logging_steps=100,
    save_total_limit=1,
    fp16=torch.cuda.is_available(),
    report_to="none",
    dataloader_num_workers=0,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    tokenizer=tokenizer,
    data_collator=DataCollatorWithPadding(tokenizer),
    compute_metrics=compute_metrics,
)

# –¶–∏–∫–ª –ø–æ —ç–ø–æ—Ö–∞–º —Å —Ä—É—á–Ω–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π
for epoch in range(int(EPOCHS)):
    print(f"\nüîπ –≠–ø–æ—Ö–∞ {epoch + 1}/{EPOCHS}")
    trainer.train()
    print("üß™ –í–∞–ª–∏–¥–∞—Ü–∏—è...")
    metrics = trainer.evaluate()
    print(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã: Accuracy = {metrics['eval_accuracy']:.4f}, F1 = {metrics['eval_f1_macro']:.4f}")

# –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å
trainer.save_model(MODEL_OUTPUT_DIR)
tokenizer.save_pretrained(MODEL_OUTPUT_DIR)

print(f"\n‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {MODEL_OUTPUT_DIR}")
print("üéØ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")